{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib scikit-learn spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read usernames from a CSV file with a header\n",
    "df = pd.read_csv('usernames_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of special characters in a username\n",
    "def count_special_characters(username):\n",
    "    special_characters = re.findall(r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-\\.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]', username)\n",
    "    return len(special_characters)\n",
    "\n",
    "# Function to get a list of unique special characters in a username\n",
    "def unique_special_characters(username):\n",
    "    special_characters = re.findall(r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-\\.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]', username)\n",
    "    return list(set(special_characters))\n",
    "\n",
    "# Function to count the number of nouns in a username\n",
    "def count_nouns(username):\n",
    "    doc = nlp(username)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Engineering\n",
    "df['Length'] = df['Username'].apply(len)\n",
    "df['Special Characters'] = df['Username'].apply(lambda username: int(bool(re.search(r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-\\.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]', username))))\n",
    "df['Number of Special Characters'] = df['Username'].apply(count_special_characters)\n",
    "df['Unique Special Characters'] = df['Username'].apply(unique_special_characters)\n",
    "df['Numbers'] = df['Username'].apply(lambda username: int(bool(re.search(r'\\d', username))))\n",
    "# df['Uppercase Letters'] = df['Username'].apply(lambda username: sum(1 for char in username if char.isupper()))\n",
    "df['Number of Words'] = df['Username'].apply(lambda username: len(re.findall(r'\\w+', username)))\n",
    "# df['Nouns'] = df['Username'].apply(count_nouns)\n",
    "\n",
    "\n",
    "unique_chars = df['Username'].apply(unique_special_characters)\n",
    "all_unique_chars = set(char for sublist in unique_chars for char in sublist)\n",
    "\n",
    "# Create binary columns for unique special characters\n",
    "for char in all_unique_chars:\n",
    "    df[char] = df['Username'].apply(lambda username: int(char in unique_special_characters(username)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize and scale features\n",
    "scaler = StandardScaler()\n",
    "# scaled_features = scaler.fit_transform(df[['Length', 'Special Characters', 'Numbers', 'Uppercase Letters', 'Number of Words', 'Nouns', 'Number of Special Characters'] + list(all_unique_chars)])\n",
    "scaled_features = scaler.fit_transform(df[['Length', 'Special Characters', 'Numbers', 'Number of Words', 'Number of Special Characters'] + list(all_unique_chars)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate cosine similarity matrix (since we have text-based features)\n",
    "cosine_sim = cosine_similarity(scaled_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(cosine_sim, method='average', metric='cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dendrogram (same as before)\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, labels=df['Username'].tolist(), leaf_rotation=45, leaf_font_size=12)\n",
    "plt.xlabel('Usernames')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Dendrogram of Usernames')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cut the dendrogram to get clusters (same as before)\n",
    "num_clusters = 7  # Change this value based on your analysis\n",
    "clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Silhouette Score\n",
    "silhouette_avg = silhouette_score(scaled_features, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree classifier to predict clusters\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(scaled_features, clusters)  # 'clusters' are the obtained cluster labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importances = classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the most important features for each cluster\n",
    "cluster_most_important_features = {}\n",
    "\n",
    "for cluster_label in np.unique(clusters):\n",
    "    # Get instances that belong to the current cluster\n",
    "    cluster_instances = df[clusters == cluster_label]\n",
    "    \n",
    "    # Get the most important features for the current cluster using the trained classifier\n",
    "    important_features = df.columns[np.argsort(-feature_importances)]\n",
    "    most_important_features = important_features[np.isin(important_features, cluster_instances.columns)]\n",
    "    \n",
    "    # Store the most important features in the dictionary\n",
    "    cluster_most_important_features[cluster_label] = most_important_features\n",
    "\n",
    "# Create a new column in the DataFrame with the most important features for each cluster\n",
    "df['Most Important Features'] = df['Cluster'].map(cluster_most_important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store cluster-wise feature importances\n",
    "cluster_avg_feature_importances = {}\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_label in np.unique(clusters):\n",
    "    # Get rows for the current cluster\n",
    "    cluster_rows = df[df['Cluster'] == cluster_label]\n",
    "    \n",
    "    # Get the indices of the rows in the current cluster\n",
    "    cluster_indices = cluster_rows.index\n",
    "    \n",
    "    # Filter the feature importances based on the current cluster's indices\n",
    "    cluster_feature_importances = feature_importances[cluster_indices]\n",
    "    \n",
    "    # Calculate the average feature importances for the current cluster\n",
    "    avg_importances = np.mean(cluster_feature_importances, axis=0)\n",
    "    \n",
    "    # Store the average importances in the dictionary\n",
    "    cluster_avg_feature_importances[cluster_label] = avg_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances)\n",
    "plt.xticks(range(len(df.columns)), df.columns, rotation='vertical')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.tight_layout()  # Ensure proper spacing of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the clustered usernames (same as before)\n",
    "print(df)\n",
    "\n",
    "df.to_csv('clustered_usernames.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze the patterns within each cluster (same as before)\n",
    "for cluster_id in range(1, num_clusters + 1):\n",
    "    cluster_data = df[df['Cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} - Usernames: [{len(cluster_data['Username'].tolist())}]\")\n",
    "    print(cluster_data['Username'].tolist())\n",
    "    # Additional analysis can be performed on each cluster, such as examining naming conventions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
